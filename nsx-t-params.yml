enable_ansible_debug: false # set value to true for verbose output from ansible
nsx_t_installer: nsx-t-gen  # Set to name of installer or env or any value so resources can be identified

# vCenter details to deploy the Mgmt OVAs (Mgr, Edge, Controller)
vcenter_host: vlab-vcsa.vballin.com       # EDIT - this is for deployment of the ovas for the                                                                                           mgmt plane
vcenter_usr: administrator@vsphere.local  # EDIT - this is for deployment of the ovas for                                                                                           the mgmt plane
vcenter_pwd: VMware1!               # EDIT - this is for deployment of the ovas for the                                                                                           mgmt plane
vcenter_datacenter: vlab-dc            # EDIT
vcenter_datastore: vlab-nfs-ds-ssd              # EDIT
vcenter_cluster: vlab-mgmt                  # EDIT
vcenter_manager: vlab-vcsa.vballin.com                # EDIT
vcenter_rp:                               #EDIT resource pool where mgmt VMs would be deployed


# OVA general network settings
ntpservers: us.pool.ntp.org                    # EDIT
mgmt_portgroup: 'vlab-mgmt'              # EDIT
dnsserver: 192.168.64.60                     # EDIT
dnsdomain: vballin.com                  # EDIT
defaultgateway: 192.168.64.1                # EDIT
netmask: 255.255.255.0                    # EDIT

# Download NSX-T 2.1 bits from
# https://my.vmware.com/group/vmware/details?downloadGroup=NSX-T-210&productId=673
# Host a Webserver to serve the ova images and ovftool
# Please edit the endpoint and the file names
nsx_image_webserver: http://172.18.0.2
nsx_mgr_ova: nsx-unified-appliance-2.1.0.0.0.7395503.ova
nsx_controller_ova: nsx-controller-2.1.0.0.0.7395493.ova
nsx_edge_ova: nsx-edge-2.1.0.0.0.7395502.ova
ovftool_image: VMware-ovftool-4.2.0-5965791-lin.x86_64.bundle

# The Esxi Hosts can be added to transport nodes in two ways:
# a) specify the esxi hosts individually - first checked for
# b) or use compute_vcenter_manager to add hosts under a specific vcenter as transport                                                                                           nodes

# Specify passwd of the esxi hosts that should be used for nsx-t
esxi_hosts_root_pwd: netmanftw  # EDIT - Root password for the esxi hosts

# Specify FQDN, ip (and passwd if they are different) for each of the esxi hosts that s                                                                                          hould be used for nsx-t
esxi_hosts_config: 
 
# esxi_hosts:
#  - name: esxi-host1.corp.local.io
#    ip: 10.13.12.10
#    root_pwd: rootPasswd
# - name: esxi-host2.corp.local.io
#   ip: 10.13.12.11
#   root_pwd: rootPasswd

compute_vcenter_configs: |
  compute_vcenters: 
  - compute_cluster_name: vlab-cna
    vcenter_host: vlab-vcsa.vballin.com
    vcenter_usr: administrator@vsphere.local
    vcenter_pwd: VMware1!
    vcenter_cluster: vlab-cna
    overlay_profile_mtu: 1600 # Min 1600
    overlay_profile_vlan: 70 # VLAN ID for the TEP/Overlay network
    # Specify an unused vmnic on esxi host to be used for nsx-t
    # can be multiple vmnics separated by comma
    esxi_vmnics: vmnic1 # vmnic1,vmnic2...

  - compute_cluster_name: vlab-cna
    vcenter_host: vlab-vcsa.vballin.com
    vcenter_usr: administrator@vsphere.local
    vcenter_pwd: VMware1!
    vcenter_cluster: vlab-mgmt
    overlay_profile_mtu: 1600 # Min 1600
    overlay_profile_vlan: 70 # VLAN ID for the TEP/Overlay network
    # Specify an unused vmnic on esxi host to be used for nsx-t
    # can be multiple vmnics separated by comma
    esxi_vmnics: vmnic1 # vmnic1,vmnic2...

compute_vcenter_cluster:
compute_vcenter_host:
compute_vcenter_manager:
compute_vcenter_pwd:
compute_vcenter_usr:

# Use following section to allow different clusters and resource pool per controller
nsx_t_controllers_config: 
  
# Using a separate vCenter to add Edges
# Edge Specific vCenter settings
# If Edges are going to use the same vCenter as Mgr, then dont set any of the following properties
edge_vcenter_host: vlab-vcsa.vballin.com #    EDIT - If filled, then Edges would use this separate vcenter
edge_vcenter_usr: administrator@vsphere.local #     EDIT -  Use Edge specific vCenter
edge_vcenter_pwd: VMware1!  #     EDIT -  Use Edge specific vCenter
edge_vcenter_datacenter: vlab-dc #     EDIT -  Use Edge specific vCenter
edge_vcenter_datastore:  vlab-nfs-ds-ssd #     EDIT -  Use Edge specific vCenter
edge_vcenter_cluster: vlab-mgmt #     EDIT -  Use Edge specific vCenter
edge_ntpservers: us.pool.ntp.org #     EDIT -  Use Edge specific vCenter
edge_mgmt_portgroup: 'vlab-mgmt' #     EDIT -  Use Edge specific vCenter
edge_dnsserver: 192.168.64.60 #     EDIT -  Use Edge specific vCenter
edge_dnsdomain: vballin.com #     EDIT -  Use Edge specific vCenter
edge_defaultgateway: 192.168.64.1 #     EDIT -  Use Edge specific vCenter
edge_netmask: 255.255.255.0 #     EDIT -  Use Edge specific vCenter


# Edit fololowing parameters
nsx_t_manager_host_name: vlab-nsxmgr.vballin.com # Set as FQDN, will be used also as certifi                                                                                          cate common name
nsx_t_manager_vm_name: 'vlab-nsxmgr'               # Can have spaces
nsx_t_manager_ip: 192.168.64.20
nsx_t_manager_admin_user: admin
nsx_t_manager_admin_pwd: VMware1!         # Min 8 chars, upper, lower, number, special                                                                                           digit
nsx_t_manager_root_pwd: VMware1!          # Min 8 chars, upper, lower, number, special                                                                                           digit
nsx_t_controller_host_prefix: nsx-t-ctl   # Without spaces, Generated controller would                                                                                           be nsx-t-ctrl-1.corp.local.io,...
nsx_t_controller_vm_name_prefix: 'vlab-nsx-ctrl' # Generated edge host name would be                                                                                           "NSX-T Controller-1"
nsx_t_controller_ips: 192.168.64.21,192.168.64.22,192.168.64.23 # Should be 1 or 3 i                                                                                          ps to maintain quorum for Controller Cluster
nsx_t_controller_root_pwd: VMware1!       # Min 8 chars, upper, lower, number, special                                                                                           digit
nsx_t_controller_cluster_pwd: VMware1!    # Min 8 chars, upper, lower, number, special                                                                                           digit
nsx_t_edge_host_prefix: nsx-t-edge        # Without spaces, generated edge would be nsx                                                                                          -t-edge-1.corp.local.io,...
nsx_t_edge_vm_name_prefix: 'vlab-nsx-edge'   # Generated edge host name would be "NSX-T Ed                                                                                          ge-1"
nsx_t_edge_ips: 192.168.64.24,192.168.64.25   # comma separated ips, requires min 2 f                                                                                          or HA
nsx_t_edge_root_pwd: VMware1!
nsx_t_edge_portgroup_ext: nsx-uplink        # For external routing
nsx_t_edge_portgroup_transport: nsx-transport  # For TEP/overlay

# If ova deployment succeeded but controller membership failed or edges didnt get to jo                                                                                          in for any reason
# enable rerun of the configure controllers
rerun_configure_controllers: false  # set it to true if you want to rerun the configure                                                                                           controllers
                                    # (as part of base ova install job) even as ova dep                                                                                          loyment succeeded

# Edge network interfaces
# Network1 and Network4 are for mgmt and not used for uplink
# Network2 is for external uplink
# Network3 is for overlay
# Change only if necessary
nsx_t_edge_overlay_interface: fp-eth1 # Wired to Network3
nsx_t_edge_uplink_interface: fp-eth0  # Wired to Network2

# Tunnel endpoint network ip pool - change pool_end based on # of members in the tep po                                                                                          ol
nsx_t_tep_pool_name: ip-pool-teps
nsx_t_tep_pool_cidr: 192.168.70.0/24
nsx_t_tep_pool_gateway: 192.168.70.1
nsx_t_tep_pool_start: 192.168.70.10
nsx_t_tep_pool_end: 192.168.70.200
nsx_t_tep_pool_nameserver: 192.168.64.60

# Memory reservation is turned ON by default with the NSX-T OVAs.
# This would mean a deployment of an edge or a mgr would reserve full memory
# leading to memory constraints
# if nsx_t_keep_reservation to true  - would keep reservation ON, recommended for produ                                                                                          ction setups.
# if nsx_t_keep_reservation to false - would turn reservation OFF, recommended for POCs                                                                                          , smaller setups.
nsx_t_keep_reservation: true # for Prod setup

# Default Sizing of NSX-T VMs
# Manager Sizing
# small  : 2 vCPU,  8GB RAM
# medium : 4 vCPU, 16GB RAM - default with OVA
# large  : 8 vCPU, 32GB RAM
# Controller Sizing
# default: 4 vCPU, 16GB RAM
# Edge Instance Sizing
# small  : 2 vCPU,  4GB RAM - Too small - dont use
# medium : 4 vCPU,  8GB RAM - default with OVA
# large  : 8 vCPU, 16GB RAM

# Controller is fixed to the 4 vCPU and 16 GB RAM
# Change default size for deployment for edge and mgr
# Size of the edge determines size of loadbalancers and # of lbrs.
# So, choose the size of edge based on requirements
nsx_t_mgr_deploy_size: small   # Recommended for real barebones demo, smallest setup
nsx_t_edge_deploy_size: medium # Recommended for POCs, smaller setup (# of lbrs very l                                                                                          imited)
#nsx_t_edge_deploy_size: large  # Recommended when 4 small lbrs are required

# More control over memory and vcpu to use
# Not exposed and support removed
# nsx_t_sizing_spec: |
#   nsx_t_sizing:
#     mgr:
#       cpu: 2
#       memory: 8192 # in MB, needs to be multiples of 1024
#     controller:
#       # Controller would be resized from 4vCPU/16GB to 2vCPU/8GB
#       cpu: 2
#       memory: 8192 # in MB, needs to be multiples of 1024
#     edge:
#       cpu: 2
#       memory: 8192 # in MB, needs to be multiples of 1024


nsx_t_overlay_hostswitch: hs-overlay
nsx_t_vlan_hostswitch: hs-vlan

# For Edge External uplink
nsx_t_transport_vlan: 70

nsx_t_vlan_transport_zone: tz-vlan
nsx_t_overlay_transport_zone: tz-overlay

nsx_t_pas_ncp_cluster_tag: pks1

nsx_t_edge_cluster: 'edge-cluster-pks'

# For outbound uplink connection used by Edge
nsx_t_single_uplink_profile_name: "single-uplink-profile"
nsx_t_single_uplink_profile_mtu: 1600 # Min 1600
nsx_t_single_uplink_profile_vlan: 70 # Default

# For internal overlay connection used by Esxi hosts
nsx_t_overlay_profile_name: "host-overlay-profile"
nsx_t_overlay_profile_mtu: 1600 # Min 1600
nsx_t_overlay_profile_vlan: 70

# Specify an unused vmnic on esxi host to be used for nsx-t
# can be multiple vmnics separated by comma
nsx_t_esxi_vmnics: vmnic1 # vmnic1,vmnic2...

# Configs for T0Router (only one per run), T1Routers, Logical switches and tags...
# Make sure the ncp/cluster tag matches the one defined at the top level.
# Expects to use atleast 2 edge instances for HA
nsx_t_t0router_spec: |
  t0_router:
    name: t0-pks
    edge_indexes:
      primary: 1   # Index for primary edge to be used
      secondary: 2 # Index for secondary edge to be used
    ha_mode: 'ACTIVE_STANDBY'
    ip1: 192.168.71.8/24
    ip2: 192.168.71.9/24
    vip: 192.168.71.10/24
    vlan_uplink: 71
    static_route:
      next_hop: 192.168.71.1
      network: 0.0.0.0/0
      admin_distance: 1
    tags:
      ncp/cluster: 'pks' # Should match the top level ncp/cluster tag value
      ncp/shared_resource: 'true' # required for PKS
      testtag: 'testpks1'


# T1 Logical Router with associated logical switches
# Add additional or comment off unnecessary t1 routers and switches as needed
# Can have 3 different setups:
# 1: One shared mgmt T1 Router and infra logical switch for both PKS & PAS
# 2: One mgmt T1 Router and infra logical switch for either PKS or PAS..
#    Comment off the T1 router not required
# 3: Separate mgmt T1 Router and infra logical switch for each PKS and PAS..
#    Add additional T1Router-Mgmt2 as needed with its infra logical switch
# Name the routers and logical switches and cidrs differently to avoid conflict
#nsx_t_t1router_logical_switches_spec:
nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  # Comment off the following T1 Routers if there is no PKS
  - name: t1-pks-mgmt
    switches:
    - name: ls-pks-mgmt
      logical_switch_gw: 172.31.0.1 # Last octet should be 1 rather than 0
      subnet_mask: 24
  - name: t1-pks-service
    switches:
    - name: ls-pks-service
      logical_switch_gw: 172.31.2.1 # Last octet should be 1 rather than 0
      subnet_mask: 23

# Make sure the ncp/cluster tag matches the one defined on the T0 Router
# Additional the ncp/ha tag should be set for HA Spoof guard profile
#nsx_t_ha_switching_profile_spec: 
nsx_t_ha_switching_profile_spec: |
  ha_switching_profiles:
  - name: HASwitchingProfile
    tags:
      testtag: 'testpks1'

# Make sure the ncp/cluster tag matches the one defined on the T0 Router
# Add additional container ip blocks as needed
nsx_t_container_ip_block_spec: |
  container_ip_blocks:
  - name: ip-block-pods-deployments
    cidr: 172.16.0.0/16
    tags:
      ncp/shared_resource: 'true' # required for PKS
  - name: ip-block-nodes-deployments
    cidr: 172.25.0.0/16
    tags:
      ncp/shared_resource: 'true' # required for PKS

# Make sure the ncp/shared tag is set to true for PKS
# Additional the ncp/external tag should be set for external facing ip pool
# Add additional exernal ip pools as needed
nsx_t_external_ip_pool_spec: |
  external_ip_pools:
  - name: ip-pool-vips
    cidr: 192.168.75.0/24
    gateway: 192.168.75.1
    start: 192.168.75.10 # Should not include gateway
    end: 192.168.75.254  # Should not include gateway


# Specify NAT rules
# Provide matching dnat and snat rule for specific vms by using ips for destination_net work and translated_network that need to be exposed like Ops Mgr
# Provide snat rules for outbound from either container or a vm by specifying the source_network (cidr) and translated network ip
nsx_t_nat_rules_spec: |
  nat_rules:
  # Sample entry for PKS MGMT and Service networks
  - t0_router: t0-pks
    nat_type: snat
    source_network: 172.31.0.0/23      # PKS Clusters network cidr
    translated_network: 192.168.76.1      # SNAT External Address for PKS networks
    rule_priority: 1024                  # Higher priority

  # Sample entry for allowing inbound to PKS Ops manager
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.2     # External IP address for PKS opsmanager
    translated_network: 172.31.0.2     # Internal IP of PKS Ops manager
    rule_priority: 1024                  # Higher priority
  
  # Sample entry for allowing inbound to BOSH
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.3     # External IP address for PKS opsmanager
    translated_network: 172.31.0.3     # Internal IP of PKS Ops manager
    rule_priority: 1024                  # Higher priority
    
  # Sample entry for allowing inbound to PKS Controller
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 192.168.76.4    # External IP address for PKS opsmanager
    translated_network: 172.31.0.4     # Internal IP of PKS Ops Controller
    rule_priority: 1024                  # Higher priority
  # Sample entry for allowing outbound from PKS Controller to external
#  - t0_router: DefaultT0Router
#    nat_type: snat
#    source_network: 192.168.20.4        # Internal IP of PKS controller
#    translated_network: 10.208.50.4      # External IP address for PKS controller
#    rule_priority: 1024                  # Higher priority


nsx_t_csr_request_spec: |
  csr_request:
    #common_name not required - would use nsx_t_manager_host_name
    org_name: VMware            # EDIT
    org_unit: vlab          # EDIT
    country: US                  # EDIT
    state: CO                    # EDIT
    city: Springs                     # EDIT
    key_size: 2048               # Valid values: 2048 or 3072
    algorithm: RSA               # Valid values: RSA or DSA

# LBR definition
# By default, all the server pools would use Layer 4 - TCP pass through
# No ssl termination or handling, uses default tcp active monitor & auto map for virtua                                                                                          l server
# Auto Map mode uses LB interface IP and ephemeral port.
# In scenarios where both Clients and Pool Members are attached to the same Logical Rou                                                                                          ter,
# SNAT (Auto Map or IP List) must be used.

# LBR Sizing guide
# LB Size | Virtual Servers | Pool Members
# small   |   10            |   30
# medium  |  100            |  300
# large   | 1000            | 3000

# No. of LBs per edge based on size of edge
# Edge Size | Small LBs| Medium LBs| Large LBs
#
# small     |    0     |   0       | 0
# medium    |    1     |   0       | 0 # Recommended for running only PAS or PKS
# large     |    4     |   1       | 1 # Recommended for running PAS + PKS
# Bare metal - not handled here
nsx_t_lbr_spec:
#nsx_t_lbr_spec: |
#  loadbalancers:
  # Sample entry for creating LBR for PAS ERT
#  - name: PAS-ERT-LBR
#    t1_router: T1-Router-PAS-ERT # Should match a previously declared T1 Router
#    size: small                  # Allowed sizes: small, medium, large
#    virtual_servers:
#    - name: goRouter443         # Name that signifies function being exposed
#      vip: 10.208.40.4         # Exposed VIP for LBR to listen on
#      port: 443
#      members:
#      - ip: 192.168.24.11       # Internal ip of GoRouter instance 1
#        port: 443
#    - name: goRouter80
#      vip: 10.208.40.4
#      port: 80
#      members:
#      - ip: 192.168.24.31       # Internal ip of GoRouter instance 1
#        port: 80
#      - ip: 192.168.24.32       # Internal ip of GoRouter instance 2
#        port: 80
#    - name: sshProxy            # SSH Proxy exposed to outside
#      vip: 10.208.40.5
#      port: 2222                # Port 2222 for ssh proxy
#      members:
#      - ip: 192.168.24.41       # Internal ip of Diego Brain where ssh proxy runs
#        port: 2222
